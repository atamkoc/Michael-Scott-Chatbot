{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPCZkCl2kh08XIzXoIuv0BC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"fdf8c19483ea4320b8c888924363e9b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_91626fa9d0d1436c9537b1d53760fff5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1ee38cd6ba2d430a8c4c6af47451c570","IPY_MODEL_4c57da17e8be418c8dfc406e542d8b75","IPY_MODEL_c62bdbbbe9a446aaa58db515fca9626c"]}},"91626fa9d0d1436c9537b1d53760fff5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1ee38cd6ba2d430a8c4c6af47451c570":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_32bf336850ee4672a863c2e668e384db","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Epoch: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_767412dc28f24765b210bd2743e0664d"}},"4c57da17e8be418c8dfc406e542d8b75":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1216c8815bb14f87bdec5fd8878136d7","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":4,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1b03b10d31554757ad97c6729a3c1f8d"}},"c62bdbbbe9a446aaa58db515fca9626c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d13cf3a067b848d9903e2b8bd20dbd21","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4/4 [1:15:39&lt;00:00, 1128.16s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3a042cc3590a4f3e90b6871c3c38c700"}},"32bf336850ee4672a863c2e668e384db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"767412dc28f24765b210bd2743e0664d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1216c8815bb14f87bdec5fd8878136d7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1b03b10d31554757ad97c6729a3c1f8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d13cf3a067b848d9903e2b8bd20dbd21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3a042cc3590a4f3e90b6871c3c38c700":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e9a9b5e61f2f4e9281d57207ba2ba975":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_710bd6d8ded1465ea1f786dc50135f8b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_43ca978dfaa94ca6ba6cff1e6f6f6e0a","IPY_MODEL_6862ccd007ac4d10bbbc89089d97a51a","IPY_MODEL_96cc8b05725a4a8abdbe784906671e73"]}},"710bd6d8ded1465ea1f786dc50135f8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"43ca978dfaa94ca6ba6cff1e6f6f6e0a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1cc3e691979f4d218b801705381cab1d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Iteration: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4279a849ed78474cbe2cef306864b59c"}},"6862ccd007ac4d10bbbc89089d97a51a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_16058d87ce5448548f120f0fa4c0ecbf","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":10620,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":10620,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_228ba6ca82f2485baf1d5930e1738c40"}},"96cc8b05725a4a8abdbe784906671e73":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c543bffce19749a0874a2e4ad248aa63","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 10620/10620 [19:17&lt;00:00,  9.20it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d05648a5be5d4d818fc7d0a0ef922e3c"}},"1cc3e691979f4d218b801705381cab1d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4279a849ed78474cbe2cef306864b59c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"16058d87ce5448548f120f0fa4c0ecbf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"228ba6ca82f2485baf1d5930e1738c40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c543bffce19749a0874a2e4ad248aa63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d05648a5be5d4d818fc7d0a0ef922e3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5e86a98ad638404baf52e09fdf3be896":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3eb421597e474585a2461913768c9ee2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e2de32753a66442cb701a6fe83aad91c","IPY_MODEL_c642b1bcedcb43cd99a3f601a7f3f5e3","IPY_MODEL_51588623cfb4474b8bf8d42a444295d9"]}},"3eb421597e474585a2461913768c9ee2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e2de32753a66442cb701a6fe83aad91c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_638cadcc6d464aa6937352786a8e2dd3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Iteration: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cc5361fe2a1c4560835dcd2f3e575f2f"}},"c642b1bcedcb43cd99a3f601a7f3f5e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_afcf729b57974d778775a857a6fe27b6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":10620,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":10620,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6764e5ccc1054c52bee806f3a8d00df0"}},"51588623cfb4474b8bf8d42a444295d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_35dcc3c13a4f431ba784ff10bff06de3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 10620/10620 [19:12&lt;00:00,  9.66it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2791828d08ca43d2aee0a9577ceddf4e"}},"638cadcc6d464aa6937352786a8e2dd3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cc5361fe2a1c4560835dcd2f3e575f2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"afcf729b57974d778775a857a6fe27b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6764e5ccc1054c52bee806f3a8d00df0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"35dcc3c13a4f431ba784ff10bff06de3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2791828d08ca43d2aee0a9577ceddf4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f600f29a234459d950e45e78e88224d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d44231a59672407db71cb2540c8064f2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dcee001970a04badb27fb5c3c249dd6b","IPY_MODEL_e6c34fe75a6547a49948e9b970f1e07f","IPY_MODEL_f1f6f1ee7d874fe480371de65d19a4e8"]}},"d44231a59672407db71cb2540c8064f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dcee001970a04badb27fb5c3c249dd6b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cc9e2c1f8e0f4433807b982f59c7559b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Iteration: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f8f19792cd6e48d1a248ae0671a367a0"}},"e6c34fe75a6547a49948e9b970f1e07f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5f5fe2713f10442c93454e33fe4a8920","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":10620,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":10620,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_91cb2751e2944631b43d449d5e3ea664"}},"f1f6f1ee7d874fe480371de65d19a4e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_efd3b95ae8104fe2b84438618a118f42","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 10620/10620 [18:31&lt;00:00,  9.28it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6abc6ebd2e9143fc96393d86e5ddc669"}},"cc9e2c1f8e0f4433807b982f59c7559b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f8f19792cd6e48d1a248ae0671a367a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5f5fe2713f10442c93454e33fe4a8920":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"91cb2751e2944631b43d449d5e3ea664":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"efd3b95ae8104fe2b84438618a118f42":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6abc6ebd2e9143fc96393d86e5ddc669":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a9d239a185f7443fb8cd3ba536f3bcd0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c59999d0942b4952b45201e384697894","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9001231380914348b4afea934feb2074","IPY_MODEL_4bc3a5693a82453cb192b97a583fa2b6","IPY_MODEL_bda4d2fd4b7e48debad663eb5ec598ad"]}},"c59999d0942b4952b45201e384697894":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9001231380914348b4afea934feb2074":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1c199050148a4b4bb9d0f1bcb01f4433","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Iteration: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_606e02c14ed94a348a311f12d32342d5"}},"4bc3a5693a82453cb192b97a583fa2b6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_81e5cf0372714f58855211a8c0c341a4","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":10620,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":10620,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_09bfdbd82a344a8d8205f1b8cf433829"}},"bda4d2fd4b7e48debad663eb5ec598ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_00c8eb775a5946dd91f550404e05ebd3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 10620/10620 [18:37&lt;00:00, 10.28it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a4587585fbc04d4f973f1eedc215b7fb"}},"1c199050148a4b4bb9d0f1bcb01f4433":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"606e02c14ed94a348a311f12d32342d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"81e5cf0372714f58855211a8c0c341a4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"09bfdbd82a344a8d8205f1b8cf433829":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"00c8eb775a5946dd91f550404e05ebd3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a4587585fbc04d4f973f1eedc215b7fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"79831c710e134ea9b8830a95ad623e79":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_74dafb5e1f8449dbb05d5bbdc21fc428","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6cc008309c564b8585b84e86c73f7b24","IPY_MODEL_1027c10ae2094ddab639cf4bd6699b71","IPY_MODEL_f77e9c9abb0c4c37897f96f18d62512f"]}},"74dafb5e1f8449dbb05d5bbdc21fc428":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6cc008309c564b8585b84e86c73f7b24":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_12ffe1f1d2884b3c8b18c1a9ff4a8d16","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Evaluating: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e54ebe9c50e543e9bdc399ec04eff2b1"}},"1027c10ae2094ddab639cf4bd6699b71":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f7a7a8de4ba44bd89d108cbb585a8816","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1181,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1181,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b4fa2faa26e440719d10293e5187c06e"}},"f77e9c9abb0c4c37897f96f18d62512f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_77fb5dbcf46646c2b74d41ef14809953","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1181/1181 [00:23&lt;00:00, 60.78it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_470bde09cd724070b41457af0982a91b"}},"12ffe1f1d2884b3c8b18c1a9ff4a8d16":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e54ebe9c50e543e9bdc399ec04eff2b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f7a7a8de4ba44bd89d108cbb585a8816":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b4fa2faa26e440719d10293e5187c06e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"77fb5dbcf46646c2b74d41ef14809953":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"470bde09cd724070b41457af0982a91b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"id":"lpPf5dy4_QFj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645058092744,"user_tz":-60,"elapsed":3528,"user":{"displayName":"Ali Kemal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06628100712125702404"}},"outputId":"c640a90a-892d-4645-e279-b34afc024ebc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/My Drive/Colab Notebooks\")"],"metadata":{"id":"N1ub3YGH_kwy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Nu82UIO3CMm","executionInfo":{"status":"ok","timestamp":1645058101331,"user_tz":-60,"elapsed":3530,"user":{"displayName":"Ali Kemal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06628100712125702404"}},"outputId":"4c549b97-3241-4713-935a-12d77228614b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"]}],"source":["!pip install transformers\n","import transformers"]},{"cell_type":"code","source":["# all the imports\n","\n","import glob\n","import logging\n","import os\n","import pickle\n","import random\n","import re\n","import shutil\n","from typing import Dict, List, Tuple\n","\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm.notebook import tqdm, trange\n","\n","from pathlib import Path\n","\n","from transformers import (\n","    MODEL_WITH_LM_HEAD_MAPPING,\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    PreTrainedModel,\n","    PreTrainedTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n","\n","\n","try:\n","    from torch.utils.tensorboard import SummaryWriter\n","except ImportError:\n","    from tensorboardX import SummaryWriter"],"metadata":{"id":"Ev0gFxk93EAM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df=pd.read_csv(\"the-office_lines.csv\")"],"metadata":{"id":"2NSC_U4C4UbI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head(50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"CQJqHnRr_Eg0","executionInfo":{"status":"ok","timestamp":1645058104710,"user_tz":-60,"elapsed":11,"user":{"displayName":"Ali Kemal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06628100712125702404"}},"outputId":"3f1988df-e0ba-4d49-e111-dbc4619b623b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-4ec241d7-b235-4e4e-924a-99d42c172e21\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Character</th>\n","      <th>Line</th>\n","      <th>Season</th>\n","      <th>Episode_Number</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Michael</td>\n","      <td>All right Jim. Your quarterlies look very goo...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Jim</td>\n","      <td>Oh, I told you. I couldn’t close it. So…</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Michael</td>\n","      <td>So you’ve come to the master for guidance? Is...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Jim</td>\n","      <td>Actually, you called me in here, but yeah.</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Michael</td>\n","      <td>All right. Well, let me show you how it’s don...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>Michael</td>\n","      <td>[on the phone] Yes, I’d like to speak to your...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>Michael</td>\n","      <td>I’ve, uh, I’ve been at Dunder Mifflin for 12 ...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>Pam</td>\n","      <td>Well. I don’t know.</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>Michael</td>\n","      <td>If you think she’s cute now, you should have ...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>Pam</td>\n","      <td>What?</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>Michael</td>\n","      <td>Any messages?</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>Pam</td>\n","      <td>Uh, yeah. Just a fax.</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>Michael</td>\n","      <td>Oh! Pam, this is from Corporate. How many tim...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>Pam</td>\n","      <td>You haven’t told me.</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>Michael</td>\n","      <td>It’s called the wastepaper basket! Look at th...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>Michael</td>\n","      <td>People say I am the best boss. They go, “God ...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>Dwight</td>\n","      <td>[singing] Shall I play for you? Pa rum pump u...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>Jim</td>\n","      <td>My job is to speak to clients on the phone ab...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>Michael</td>\n","      <td>Whassup!</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>Jim</td>\n","      <td>Whassup! I still love that after seven years.</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>20</td>\n","      <td>Michael</td>\n","      <td>Whassup!</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>Dwight</td>\n","      <td>Whassup!</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>22</td>\n","      <td>Michael</td>\n","      <td>Whass…up!</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>23</td>\n","      <td>Dwight</td>\n","      <td>Whassup.</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>24</td>\n","      <td>Michael</td>\n","      <td>[Strains, grunts] What?</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>25</td>\n","      <td>Jim</td>\n","      <td>Nothing.</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>26</td>\n","      <td>Michael</td>\n","      <td>OK. All right. See you later.</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>27</td>\n","      <td>Jim</td>\n","      <td>All right. Take care.</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>28</td>\n","      <td>Michael</td>\n","      <td>Back to work.</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>29</td>\n","      <td>Jan</td>\n","      <td>[on her cell phone] Just before lunch. That w...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>30</td>\n","      <td>Michael</td>\n","      <td>Corporate really doesn’t really interfere wit...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>31</td>\n","      <td>Jan</td>\n","      <td>Alright, was there anything you wanted to add...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>32</td>\n","      <td>Michael</td>\n","      <td>Um… Me no get an agenda.</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>33</td>\n","      <td>Jan</td>\n","      <td>What? I’m sorry?</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>34</td>\n","      <td>Michael</td>\n","      <td>I didn’t get any agenda.</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>35</td>\n","      <td>Jan</td>\n","      <td>Well, I faxed one over to you this morning.</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>36</td>\n","      <td>Michael</td>\n","      <td>Really? I didn’t… [looks at Pam] Did we get a...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>37</td>\n","      <td>Pam</td>\n","      <td>Uh, yeah, the one…</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>38</td>\n","      <td>Michael</td>\n","      <td>Why isn’t it in my hand? A company runs on ef...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>39</td>\n","      <td>Pam</td>\n","      <td>You put in the garbage can that was a special...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>40</td>\n","      <td>Michael</td>\n","      <td>Yeah, that was a joke. That was a joke that w...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>41</td>\n","      <td>Jan</td>\n","      <td>Do you want to look at mine?</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>42</td>\n","      <td>Michael</td>\n","      <td>Yeah, yeah. Lovely. Thank you.</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>43</td>\n","      <td>Jan</td>\n","      <td>OK. Since the last meeting, Ellen and the boa...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>44</td>\n","      <td>Michael</td>\n","      <td>OK…</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>45</td>\n","      <td>Jan</td>\n","      <td>Michael, don’t panic.</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>46</td>\n","      <td>Michael</td>\n","      <td>No, no, no, no, this is good. This is good. T...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>47</td>\n","      <td>Jan</td>\n","      <td>No, no, no, Michael, listen OK. Don’t panic....</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>48</td>\n","      <td>Michael</td>\n","      <td>All the alarm bells are kind of going… ringie...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>49</td>\n","      <td>Jan</td>\n","      <td>I’ve spoken to Josh in Stamford. I’ve told hi...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ec241d7-b235-4e4e-924a-99d42c172e21')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4ec241d7-b235-4e4e-924a-99d42c172e21 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4ec241d7-b235-4e4e-924a-99d42c172e21');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["    Unnamed: 0 Character  ... Season  Episode_Number\n","0            0   Michael  ...      1               1\n","1            1       Jim  ...      1               1\n","2            2   Michael  ...      1               1\n","3            3       Jim  ...      1               1\n","4            4   Michael  ...      1               1\n","5            5   Michael  ...      1               1\n","6            6   Michael  ...      1               1\n","7            7       Pam  ...      1               1\n","8            8   Michael  ...      1               1\n","9            9       Pam  ...      1               1\n","10          10   Michael  ...      1               1\n","11          11       Pam  ...      1               1\n","12          12   Michael  ...      1               1\n","13          13       Pam  ...      1               1\n","14          14   Michael  ...      1               1\n","15          15   Michael  ...      1               1\n","16          16    Dwight  ...      1               1\n","17          17       Jim  ...      1               1\n","18          18   Michael  ...      1               1\n","19          19       Jim  ...      1               1\n","20          20   Michael  ...      1               1\n","21          21    Dwight  ...      1               1\n","22          22   Michael  ...      1               1\n","23          23    Dwight  ...      1               1\n","24          24   Michael  ...      1               1\n","25          25       Jim  ...      1               1\n","26          26   Michael  ...      1               1\n","27          27       Jim  ...      1               1\n","28          28   Michael  ...      1               1\n","29          29       Jan  ...      1               1\n","30          30   Michael  ...      1               1\n","31          31       Jan  ...      1               1\n","32          32   Michael  ...      1               1\n","33          33       Jan  ...      1               1\n","34          34   Michael  ...      1               1\n","35          35       Jan  ...      1               1\n","36          36   Michael  ...      1               1\n","37          37       Pam  ...      1               1\n","38          38   Michael  ...      1               1\n","39          39       Pam  ...      1               1\n","40          40   Michael  ...      1               1\n","41          41       Jan  ...      1               1\n","42          42   Michael  ...      1               1\n","43          43       Jan  ...      1               1\n","44          44   Michael  ...      1               1\n","45          45       Jan  ...      1               1\n","46          46   Michael  ...      1               1\n","47          47       Jan  ...      1               1\n","48          48   Michael  ...      1               1\n","49          49       Jan  ...      1               1\n","\n","[50 rows x 5 columns]"]},"metadata":{},"execution_count":79}]},{"cell_type":"code","source":["sum(df[\"Character\"]==\"Stanley\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W-vs3pfJAVty","executionInfo":{"status":"ok","timestamp":1645058104996,"user_tz":-60,"elapsed":3,"user":{"displayName":"Ali Kemal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06628100712125702404"}},"outputId":"a5dcedcf-ca25-4f80-e227-03a92fe8675e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["750"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":["sum(df[\"Season\"]==1)+sum(df[\"Season\"]==2)+sum(df[\"Season\"]==3)+sum(df[\"Season\"]==4)\n","#df=df.head(21879)"],"metadata":{"id":"SawCTNwPr-mL","executionInfo":{"status":"ok","timestamp":1645058105282,"user_tz":-60,"elapsed":3,"user":{"displayName":"Ali Kemal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06628100712125702404"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"12317fa6-961f-44e4-da89-33737c5124a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["21879"]},"metadata":{},"execution_count":81}]},{"cell_type":"code","source":["sum(df[\"Character\"]==\"Michael\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BDCVZP-1DuMf","executionInfo":{"status":"ok","timestamp":1645058105839,"user_tz":-60,"elapsed":3,"user":{"displayName":"Ali Kemal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06628100712125702404"}},"outputId":"1ab71a64-3945-4663-9a5f-888a1f28b94c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["11806"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["sum(df[\"Character\"]==\"Pam\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_lOee9g2Eo2o","executionInfo":{"status":"ok","timestamp":1645058106098,"user_tz":-60,"elapsed":3,"user":{"displayName":"Ali Kemal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06628100712125702404"}},"outputId":"126085fe-1bc6-4237-fc20-121e9a9f7136"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5264"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","source":["sum(df[\"Character\"]==\"Dwight\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BCdVlhXUE-yw","executionInfo":{"status":"ok","timestamp":1645058106648,"user_tz":-60,"elapsed":3,"user":{"displayName":"Ali Kemal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06628100712125702404"}},"outputId":"90bd14e2-e872-4519-e5c5-ca7457e90c75"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7393"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","source":["sum(df[\"Character\"]==\"Andy\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oNfqu66IFD8R","executionInfo":{"status":"ok","timestamp":1645058106954,"user_tz":-60,"elapsed":3,"user":{"displayName":"Ali Kemal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06628100712125702404"}},"outputId":"ff4f4420-c5e7-420c-e27a-0972a95aca08"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3933"]},"metadata":{},"execution_count":85}]},{"cell_type":"code","source":["#The character Michael has 11806 total lines in this whole script (9 seasons) script\n","#For 4 seasons Michael has 6376 total lines"],"metadata":{"id":"5_p34xOBAgDz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.drop(\"Season\",axis=1,inplace=True)"],"metadata":{"id":"3QdSbhsiAhgP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.drop(\"Episode_Number\",axis=1,inplace=True)"],"metadata":{"id":"Z5Lv5cEJAi0s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.drop(\"Unnamed: 0\",axis=1,inplace=True)"],"metadata":{"id":"1rQUdyjnAkuM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"VmsQH84oAltC","executionInfo":{"status":"ok","timestamp":1645058109838,"user_tz":-60,"elapsed":5,"user":{"displayName":"Ali Kemal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06628100712125702404"}},"outputId":"48d5b7f0-9e33-4fdb-d737-b00221fef9ac"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-e36be257-9e58-4978-bcb3-4026295add78\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Character</th>\n","      <th>Line</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Michael</td>\n","      <td>All right Jim. Your quarterlies look very goo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Jim</td>\n","      <td>Oh, I told you. I couldn’t close it. So…</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Michael</td>\n","      <td>So you’ve come to the master for guidance? Is...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Jim</td>\n","      <td>Actually, you called me in here, but yeah.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Michael</td>\n","      <td>All right. Well, let me show you how it’s don...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>58716</th>\n","      <td>Creed</td>\n","      <td>It all seems so very arbitrary. I applied for...</td>\n","    </tr>\n","    <tr>\n","      <th>58717</th>\n","      <td>Meredith</td>\n","      <td>I just feel lucky that I got a chance to shar...</td>\n","    </tr>\n","    <tr>\n","      <th>58718</th>\n","      <td>Phyllis</td>\n","      <td>I’m happy that this was all filmed so I can r...</td>\n","    </tr>\n","    <tr>\n","      <th>58719</th>\n","      <td>Jim</td>\n","      <td>I sold paper at this company for 12 years. My...</td>\n","    </tr>\n","    <tr>\n","      <th>58720</th>\n","      <td>Pam</td>\n","      <td>I thought it was weird when you picked us to ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>58721 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e36be257-9e58-4978-bcb3-4026295add78')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e36be257-9e58-4978-bcb3-4026295add78 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e36be257-9e58-4978-bcb3-4026295add78');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      Character                                               Line\n","0       Michael   All right Jim. Your quarterlies look very goo...\n","1           Jim           Oh, I told you. I couldn’t close it. So…\n","2       Michael   So you’ve come to the master for guidance? Is...\n","3           Jim         Actually, you called me in here, but yeah.\n","4       Michael   All right. Well, let me show you how it’s don...\n","...         ...                                                ...\n","58716     Creed   It all seems so very arbitrary. I applied for...\n","58717  Meredith   I just feel lucky that I got a chance to shar...\n","58718   Phyllis   I’m happy that this was all filmed so I can r...\n","58719       Jim   I sold paper at this company for 12 years. My...\n","58720       Pam   I thought it was weird when you picked us to ...\n","\n","[58721 rows x 2 columns]"]},"metadata":{},"execution_count":90}]},{"cell_type":"code","source":["# dataframe is all cleaned and we have only characters and lines now"],"metadata":{"id":"r2bhB48_Am90"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df=df.rename(columns={\"Character\":\"name\",\"Line\":\"line\"})\n","for item in df:\n","  print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KpfIhM55AsiM","executionInfo":{"status":"ok","timestamp":1645058111122,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ali Kemal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06628100712125702404"}},"outputId":"e33c681b-f1ad-46e4-ab7f-2dc7de415b7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["name\n","line\n"]}]},{"cell_type":"code","source":["CHARACTER_NAME=\"Michael\""],"metadata":{"id":"l3TeWNHIAvK8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["contexted = []\n","\n","# context window of size 7\n","n = 7\n","\n","for i in df[df.name == CHARACTER_NAME].index:\n","  if i < n:\n","    continue\n","  row = []\n","  prev = i - 1 - n # we additionally substract 1, so row will contain current responce and 7 previous responces  \n","  for j in range(i, prev, -1):\n","    row.append(df.line[j])\n","  contexted.append(row)\n","\n","columns = ['response', 'context'] \n","columns = columns + ['context/' + str(i) for i in range(n - 1)]\n","\n","df = pd.DataFrame.from_records(contexted, columns=columns)\n"],"metadata":{"id":"qJKFu2sFAwgE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.sample(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":685},"id":"Jwf36Lr9Axpy","executionInfo":{"status":"ok","timestamp":1645058115618,"user_tz":-60,"elapsed":335,"user":{"displayName":"Ali Kemal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06628100712125702404"}},"outputId":"66fd3d11-3ef2-40f2-b596-50d375d4d611"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-37721c7c-56bb-48a1-bc55-8cb005dd1491\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>response</th>\n","      <th>context</th>\n","      <th>context/0</th>\n","      <th>context/1</th>\n","      <th>context/2</th>\n","      <th>context/3</th>\n","      <th>context/4</th>\n","      <th>context/5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>7820</th>\n","      <td>Hey hey… what’s up Chuck?</td>\n","      <td>Here’s the thing.  What I wish for you is th...</td>\n","      <td>Oh, agreed, mmm, except…</td>\n","      <td>And you are not going to, either.</td>\n","      <td>Obviously.</td>\n","      <td>I am not gonna do this.</td>\n","      <td>Alright, Michael Scott Paper Company. You wa...</td>\n","      <td>Okay. [keeps eyes open]</td>\n","    </tr>\n","    <tr>\n","      <th>533</th>\n","      <td>Nah, it’s not. it’s spppplllibbb</td>\n","      <td>Regular coffee is fine.</td>\n","      <td>It’s like a slang for Starbucks. They’re all ...</td>\n","      <td>What?</td>\n","      <td>Wait a second. I should have spotted another ...</td>\n","      <td>I guess a cup of coffee would be great.</td>\n","      <td>So if you need anything else, something to ma...</td>\n","      <td>This is my conference room. So please, uh, ma...</td>\n","    </tr>\n","    <tr>\n","      <th>10731</th>\n","      <td>[in an old man mobster voice] Hey. Hey. I got...</td>\n","      <td>Sunday church service… it’s been a few years....</td>\n","      <td>[handing out cards] For all your paper and pr...</td>\n","      <td>Does the Nard-dog want Nard-pups? Yeah. I wan...</td>\n","      <td>[looking at Cece, under his breath] Ah, man. ...</td>\n","      <td>I invited everyone in the office because it’s...</td>\n","      <td>Oh, yes. [mimicking smoking] Doobie-doobie do...</td>\n","      <td>Hope you brought your pipes. We’re about to s...</td>\n","    </tr>\n","    <tr>\n","      <th>5026</th>\n","      <td>Oh hey, I know you… Elizabeth?</td>\n","      <td>Hello Michael.</td>\n","      <td>He’s happy because he’s insane. You know what...</td>\n","      <td>Look how happy he is.</td>\n","      <td>No, don’t give him… just, did you hear anythi...</td>\n","      <td>[feeding a squirrel] I’m giving him a peanut.</td>\n","      <td>OK everybody, listen up. Thank you for coming...</td>\n","      <td>I don’t know what your deal is, but he’s mine...</td>\n","    </tr>\n","    <tr>\n","      <th>8051</th>\n","      <td>[Michael, Ryan and Pam sitting at restaurant,...</td>\n","      <td>Damn it.</td>\n","      <td>I had fish yesterday.</td>\n","      <td>Cooper’s.</td>\n","      <td>Let me take you and your whole company out fo...</td>\n","      <td>I do too.</td>\n","      <td>I want a truce.</td>\n","      <td>If you want a truce, I will give you a truce.</td>\n","    </tr>\n","    <tr>\n","      <th>9478</th>\n","      <td>What is the world’s largest ocean?</td>\n","      <td>Yes.</td>\n","      <td>[entering on a Segway Scooter] I see you’ve m...</td>\n","      <td>Hello.</td>\n","      <td>[Dwight using a robotic voice over the PA sys...</td>\n","      <td>Merry Christmas Erin! Take it away boys! [dan...</td>\n","      <td>Twelve drummers drumming. [marching drum band...</td>\n","      <td>Oh my God!</td>\n","    </tr>\n","    <tr>\n","      <th>4387</th>\n","      <td>No!</td>\n","      <td>Michael—</td>\n","      <td>I got you… jade earrings.</td>\n","      <td>Oh, God.</td>\n","      <td>Six percent?  After all we’ve been through?</td>\n","      <td>Right now we can offer you a 6% raise.</td>\n","      <td>Pippity poppity.</td>\n","      <td>Hi, Toby. [clears throat] First— [Michael cle...</td>\n","    </tr>\n","    <tr>\n","      <th>1934</th>\n","      <td>Thank you.  Did you get all dark meat like I ...</td>\n","      <td>Here you go.</td>\n","      <td>Where is my cornbread?</td>\n","      <td>What is wrong with</td>\n","      <td>Yes, you did.  What is wrong with you?</td>\n","      <td>No, I didn’t.</td>\n","      <td>You just said “part of your duties are to” so...</td>\n","      <td>What?</td>\n","    </tr>\n","    <tr>\n","      <th>2452</th>\n","      <td>Yeah.</td>\n","      <td>Michael?</td>\n","      <td>He’s kidding. Dwight was kidding and I don’t ...</td>\n","      <td>This is karma because of what he did to Jenni...</td>\n","      <td>Yes.</td>\n","      <td>Are we out of jobs?</td>\n","      <td>What the hell is going on here?</td>\n","      <td>He will never act again. Also, this branch is...</td>\n","    </tr>\n","    <tr>\n","      <th>10194</th>\n","      <td>Yeah, I just remembered that I have to go to ...</td>\n","      <td>I am, but I, I gave my clubs away. I swear to...</td>\n","      <td>[to Michael] You’re gonna wanna look at the d...</td>\n","      <td>Yeah I think it’ll be a nice trip. We’re gonn...</td>\n","      <td>I love [leans over Michael’s desk to look at ...</td>\n","      <td>Okay, weirdo.</td>\n","      <td>[loudly] I need you to sign this! So bad!</td>\n","      <td>Oh, my God. Look at how cheap street level ro...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37721c7c-56bb-48a1-bc55-8cb005dd1491')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-37721c7c-56bb-48a1-bc55-8cb005dd1491 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-37721c7c-56bb-48a1-bc55-8cb005dd1491');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                response  ...                                          context/5\n","7820                           Hey hey… what’s up Chuck?  ...                            Okay. [keeps eyes open]\n","533                     Nah, it’s not. it’s spppplllibbb  ...   This is my conference room. So please, uh, ma...\n","10731   [in an old man mobster voice] Hey. Hey. I got...  ...   Hope you brought your pipes. We’re about to s...\n","5026                      Oh hey, I know you… Elizabeth?  ...   I don’t know what your deal is, but he’s mine...\n","8051    [Michael, Ryan and Pam sitting at restaurant,...  ...      If you want a truce, I will give you a truce.\n","9478                  What is the world’s largest ocean?  ...                                         Oh my God!\n","4387                                                 No!  ...   Hi, Toby. [clears throat] First— [Michael cle...\n","1934    Thank you.  Did you get all dark meat like I ...  ...                                              What?\n","2452                                               Yeah.  ...   He will never act again. Also, this branch is...\n","10194   Yeah, I just remembered that I have to go to ...  ...   Oh, my God. Look at how cheap street level ro...\n","\n","[10 rows x 8 columns]"]},"metadata":{},"execution_count":95}]},{"cell_type":"code","source":["trn_df, val_df = train_test_split(df, test_size=0.1)\n","trn_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"id":"5w22uSa_A5og","executionInfo":{"status":"ok","timestamp":1645058115620,"user_tz":-60,"elapsed":28,"user":{"displayName":"Ali Kemal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06628100712125702404"}},"outputId":"3ada3300-b748-41b3-dc4c-67ca2fa3ea50"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-104937b8-41d7-4614-8fa0-e78b28816a26\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>response</th>\n","      <th>context</th>\n","      <th>context/0</th>\n","      <th>context/1</th>\n","      <th>context/2</th>\n","      <th>context/3</th>\n","      <th>context/4</th>\n","      <th>context/5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2735</th>\n","      <td>Yeah. Sorry you had to hear that. It was a lo...</td>\n","      <td>Has she been on the phone the whole time?</td>\n","      <td>Okay, well, I really think that… [dial tone b...</td>\n","      <td>[on the phone] Michael, I’m gonna get going. ...</td>\n","      <td>No? okay, fine.</td>\n","      <td>It’s not me, either.</td>\n","      <td>No, no! Your instinct. That… Your gut reactio...</td>\n","      <td>Excuse me?</td>\n","    </tr>\n","    <tr>\n","      <th>9508</th>\n","      <td>They call it Scranton!</td>\n","      <td>in the Electric City!</td>\n","      <td>So check out how we live</td>\n","      <td>Yo, Mike, our town is dope and pretty.</td>\n","      <td>[extended Lazy Scranton video] Sittin’ in my ...</td>\n","      <td>Shut up. Shut up.</td>\n","      <td>wants to live.</td>\n","      <td>Shut –</td>\n","    </tr>\n","    <tr>\n","      <th>8326</th>\n","      <td>Welcome, welcome! Cafe Disco. I am Michael Sc...</td>\n","      <td>Oh yeah. you’ve got a knot in your crest. Thi...</td>\n","      <td>Right… mmmm… right there.</td>\n","      <td>Tell me where it hurts.</td>\n","      <td>That feels good, Dwight.</td>\n","      <td>It’s better than I imagined it!</td>\n","      <td>I hear it, too, Boss.</td>\n","      <td>Cafe Disco is dead but I can still hear the m...</td>\n","    </tr>\n","    <tr>\n","      <th>9719</th>\n","      <td>Thank you!  Wish me luck!</td>\n","      <td>Good luck, Pam!</td>\n","      <td>Good luck!</td>\n","      <td>Here we go!</td>\n","      <td>Okay, I can’t find my keys!  I cannot find m...</td>\n","      <td>[as he holds up the tape measure with his in...</td>\n","      <td>I didn’t know we had a tape measure.</td>\n","      <td>No, no, no.  Dwight, let Jim do that, please.</td>\n","    </tr>\n","    <tr>\n","      <th>7850</th>\n","      <td>[yelling from Parking lot] Alright then ever...</td>\n","      <td>No, no no no.  You’re done, Michael.</td>\n","      <td>[to everyone] Well here we are… I would just...</td>\n","      <td>I always thought Michael got a bad rap. He’s...</td>\n","      <td>Come on man, let’s, let’s go.</td>\n","      <td>Hank?  You really think Hank is going to be ...</td>\n","      <td>Okay Michael.</td>\n","      <td>not if you’re starting your own paper compan...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-104937b8-41d7-4614-8fa0-e78b28816a26')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-104937b8-41d7-4614-8fa0-e78b28816a26 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-104937b8-41d7-4614-8fa0-e78b28816a26');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                               response  ...                                          context/5\n","2735   Yeah. Sorry you had to hear that. It was a lo...  ...                                         Excuse me?\n","9508                             They call it Scranton!  ...                                            Shut – \n","8326   Welcome, welcome! Cafe Disco. I am Michael Sc...  ...   Cafe Disco is dead but I can still hear the m...\n","9719                          Thank you!  Wish me luck!  ...      No, no, no.  Dwight, let Jim do that, please.\n","7850    [yelling from Parking lot] Alright then ever...  ...    not if you’re starting your own paper compan...\n","\n","[5 rows x 8 columns]"]},"metadata":{},"execution_count":96}]},{"cell_type":"code","source":["# create dataset suitable for our model\n","def construct_conv(row, tokenizer, eos = True):\n","    flatten = lambda l: [item for sublist in l for item in sublist]\n","    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n","    conv = flatten(conv)\n","    return conv\n","\n","class ConversationDataset(Dataset):\n","    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n","\n","        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n","\n","        directory = args.cache_dir\n","        cached_features_file = os.path.join(\n","            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n","        )\n","\n","        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n","            logger.info(\"Loading features from cached file %s\", cached_features_file)\n","            with open(cached_features_file, \"rb\") as handle:\n","                self.examples = pickle.load(handle)\n","        else:\n","            logger.info(\"Creating features from dataset file at %s\", directory)\n","\n","            self.examples = []\n","            for _, row in df.iterrows():\n","                conv = construct_conv(row, tokenizer)\n","                self.examples.append(conv)\n","\n","            logger.info(\"Saving features into cached file %s\", cached_features_file)\n","            with open(cached_features_file, \"wb\") as handle:\n","                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, item):\n","        return torch.tensor(self.examples[item], dtype=torch.long)"],"metadata":{"id":"z1vJKN3BA8GQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cacheing and storing of data/checkpoints\n","\n","def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n","    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n","\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if args.n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","\n","def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n","    ordering_and_checkpoint_path = []\n","\n","    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n","\n","    for path in glob_checkpoints:\n","        if use_mtime:\n","            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n","        else:\n","            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n","            if regex_match and regex_match.groups():\n","                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n","\n","    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n","    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n","    return checkpoints_sorted\n","\n","\n","def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n","    if not args.save_total_limit:\n","        return\n","    if args.save_total_limit <= 0:\n","        return\n","\n","    # Check if we should delete older checkpoint(s)\n","    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n","    if len(checkpoints_sorted) <= args.save_total_limit:\n","        return\n","\n","    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n","    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n","    for checkpoint in checkpoints_to_be_deleted:\n","        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n","        shutil.rmtree(checkpoint)"],"metadata":{"id":"PZjZG8isA-py"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#BUILDING A MODEL"],"metadata":{"id":"_l16SZIpBD8A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelWithLMHead, AutoModelForCausalLM, AutoTokenizer\n","import torch\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n","model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-small\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XYChwmcIBFXi","executionInfo":{"status":"ok","timestamp":1645058120653,"user_tz":-60,"elapsed":4735,"user":{"displayName":"Ali Kemal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06628100712125702404"}},"outputId":"059af84a-f833-4dda-a0c1-e527894766ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:807: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n","GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n","using a masked language modeling (MLM) loss.\n","\"\"\"\n","\n","# Configs\n","logger = logging.getLogger(__name__)\n","\n","MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"],"metadata":{"id":"jLLXTl8gBIxJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Args to allow for easy convertion of python script to notebook\n","class Args():\n","    def __init__(self):\n","        self.output_dir = 'output-small'\n","        self.model_type = 'gpt2'\n","        self.model_name_or_path = 'microsoft/DialoGPT-small'\n","        self.config_name = 'microsoft/DialoGPT-small'\n","        self.tokenizer_name = 'microsoft/DialoGPT-small'\n","        self.cache_dir = 'cached'\n","        self.block_size = 512\n","        self.do_train = True\n","        self.do_eval = True\n","        self.evaluate_during_training = False\n","        self.per_gpu_train_batch_size = 1\n","        self.per_gpu_eval_batch_size = 1\n","        self.gradient_accumulation_steps = 1\n","        self.learning_rate = 5e-5\n","        self.weight_decay = 0.0\n","        self.adam_epsilon = 1e-8\n","        self.max_grad_norm = 1.0\n","        self.num_train_epochs = 4\n","        self.max_steps = -1\n","        self.warmup_steps = 0\n","        self.logging_steps = 1000\n","        self.save_steps = 3500\n","        self.save_total_limit = None\n","        self.eval_all_checkpoints = False\n","        self.no_cuda = False\n","        self.overwrite_output_dir = True\n","        self.overwrite_cache = True\n","        self.should_continue = False\n","        self.seed = 42\n","        self.local_rank = -1\n","        self.fp16 = False\n","        self.fp16_opt_level = 'O1'\n","\n","args = Args()"],"metadata":{"id":"erxqXzsRBPWt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#TRAIN AND EVALUATE"],"metadata":{"id":"xk1kkYb9BQyL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n","    \"\"\" Train the model \"\"\"\n","    if args.local_rank in [-1, 0]:\n","        tb_writer = SummaryWriter()\n","\n","    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n","\n","    def collate(examples: List[torch.Tensor]):\n","        if tokenizer._pad_token is None:\n","            return pad_sequence(examples, batch_first=True)\n","        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n","\n","    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n","    train_dataloader = DataLoader(\n","        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n","    )\n","\n","    if args.max_steps > 0:\n","        t_total = args.max_steps\n","        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n","    else:\n","        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n","\n","    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n","    model.resize_token_embeddings(len(tokenizer))\n","    # add_special_tokens_(model, tokenizer)\n","\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n","    )\n","\n","    # Check if saved optimizer or scheduler states exist\n","    if (\n","        args.model_name_or_path\n","        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n","        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n","    ):\n","        # Load in optimizer and scheduler states\n","        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n","        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n","\n","    if args.fp16:\n","        try:\n","            from apex import amp\n","        except ImportError:\n","            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n","\n","    # multi-gpu training (should be after apex fp16 initialization)\n","    if args.n_gpu > 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    # Distributed training (should be after apex fp16 initialization)\n","    if args.local_rank != -1:\n","        model = torch.nn.parallel.DistributedDataParallel(\n","            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n","        )\n","\n","    # Train!\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_dataset))\n","    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n","    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n","    logger.info(\n","        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n","        args.train_batch_size\n","        * args.gradient_accumulation_steps\n","        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n","    )\n","    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n","    logger.info(\"  Total optimization steps = %d\", t_total)\n","\n","    global_step = 0\n","    epochs_trained = 0\n","    steps_trained_in_current_epoch = 0\n","    # Check if continuing training from a checkpoint\n","    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n","        try:\n","            # set global_step to gobal_step of last saved checkpoint from model path\n","            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n","            global_step = int(checkpoint_suffix)\n","            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n","            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n","\n","            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n","            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n","            logger.info(\"  Continuing training from global step %d\", global_step)\n","            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n","        except ValueError:\n","            logger.info(\"  Starting fine-tuning.\")\n","\n","    tr_loss, logging_loss = 0.0, 0.0\n","\n","    model.zero_grad()\n","    train_iterator = trange(\n","        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n","    )\n","    set_seed(args)  # Added here for reproducibility\n","    for _ in train_iterator:\n","        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n","        for step, batch in enumerate(epoch_iterator):\n","\n","            # Skip past any already trained steps if resuming training\n","            if steps_trained_in_current_epoch > 0:\n","                steps_trained_in_current_epoch -= 1\n","                continue\n","\n","            inputs, labels = (batch, batch)\n","            if inputs.shape[1] > 1024: continue\n","            inputs = inputs.to(args.device)\n","            labels = labels.to(args.device)\n","            model.train()\n","            outputs = model(inputs, labels=labels)\n","            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n","\n","            if args.n_gpu > 1:\n","                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            tr_loss += loss.item()\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                if args.fp16:\n","                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n","                else:\n","                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","\n","                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n","                    # Log metrics\n","                    if (\n","                        args.local_rank == -1 and args.evaluate_during_training\n","                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n","                        results = evaluate(args, model, tokenizer)\n","                        for key, value in results.items():\n","                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n","                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n","                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n","                    logging_loss = tr_loss\n","\n","                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n","                    checkpoint_prefix = \"checkpoint\"\n","                    # Save model checkpoint\n","                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n","                    os.makedirs(output_dir, exist_ok=True)\n","                    model_to_save = (\n","                        model.module if hasattr(model, \"module\") else model\n","                    )  # Take care of distributed/parallel training\n","                    model_to_save.save_pretrained(output_dir)\n","                    tokenizer.save_pretrained(output_dir)\n","\n","                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n","                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n","\n","                    _rotate_checkpoints(args, checkpoint_prefix)\n","\n","                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n","                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n","                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n","\n","            if args.max_steps > 0 and global_step > args.max_steps:\n","                epoch_iterator.close()\n","                break\n","        if args.max_steps > 0 and global_step > args.max_steps:\n","            train_iterator.close()\n","            break\n","\n","    if args.local_rank in [-1, 0]:\n","        tb_writer.close()\n","\n","    return global_step, tr_loss / global_step\n","\n","# Evaluation of some model\n","\n","def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n","    # Loop to handle MNLI double evaluation (matched, mis-matched)\n","    eval_output_dir = args.output_dir\n","\n","    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n","    os.makedirs(eval_output_dir, exist_ok=True)\n","    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n","    # Note that DistributedSampler samples randomly\n","\n","    def collate(examples: List[torch.Tensor]):\n","        if tokenizer._pad_token is None:\n","            return pad_sequence(examples, batch_first=True)\n","        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n","\n","    eval_sampler = SequentialSampler(eval_dataset)\n","    eval_dataloader = DataLoader(\n","        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n","    )\n","\n","    # multi-gpu evaluate\n","    if args.n_gpu > 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    # Eval!\n","    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n","    logger.info(\"  Num examples = %d\", len(eval_dataset))\n","    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n","    eval_loss = 0.0\n","    nb_eval_steps = 0\n","    model.eval()\n","\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        inputs, labels = (batch, batch)\n","        inputs = inputs.to(args.device)\n","        labels = labels.to(args.device)\n","\n","        with torch.no_grad():\n","            outputs = model(inputs, labels=labels)\n","            lm_loss = outputs[0]\n","            eval_loss += lm_loss.mean().item()\n","        nb_eval_steps += 1\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    perplexity = torch.exp(torch.tensor(eval_loss))\n","\n","    result = {\"perplexity\": perplexity}\n","\n","    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n","    with open(output_eval_file, \"w\") as writer:\n","        logger.info(\"***** Eval results {} *****\".format(prefix))\n","        for key in sorted(result.keys()):\n","            logger.info(\"  %s = %s\", key, str(result[key]))\n","            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","\n","    return result"],"metadata":{"id":"9HnBTFWwBSI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Main runner\n","\n","def main(df_trn, df_val):\n","    args = Args()\n","    \n","    if args.should_continue:\n","        sorted_checkpoints = _sorted_checkpoints(args)\n","        if len(sorted_checkpoints) == 0:\n","            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n","        else:\n","            args.model_name_or_path = sorted_checkpoints[-1]\n","\n","    if (\n","        os.path.exists(args.output_dir)\n","        and os.listdir(args.output_dir)\n","        and args.do_train\n","        and not args.overwrite_output_dir\n","        and not args.should_continue\n","    ):\n","        raise ValueError(\n","            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n","                args.output_dir\n","            )\n","        )\n","\n","    # Setup CUDA, GPU & distributed training\n","    device = torch.device(\"cuda\")\n","    args.n_gpu = torch.cuda.device_count()\n","    args.device = device\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","        args.local_rank,\n","        device,\n","        args.n_gpu,\n","        bool(args.local_rank != -1),\n","        args.fp16,\n","    )\n","\n","    # Set seed\n","    set_seed(args)\n","\n","    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n","    model = AutoModelWithLMHead.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=False,\n","        config=config,\n","        cache_dir=args.cache_dir,\n","    )\n","    model.to(args.device)\n","    \n","    logger.info(\"Training/evaluation parameters %s\", args)\n","\n","    # Training\n","    if args.do_train:\n","        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n","\n","        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n","        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n","\n","    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n","    if args.do_train:\n","        # Create output directory if needed\n","        os.makedirs(args.output_dir, exist_ok=True)\n","\n","        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n","        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","        # They can then be reloaded using `from_pretrained()`\n","        model_to_save = (\n","            model.module if hasattr(model, \"module\") else model\n","        )  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(args.output_dir)\n","        tokenizer.save_pretrained(args.output_dir)\n","\n","        # Good practice: save your training arguments together with the trained model\n","        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","\n","        # Load a trained model and vocabulary that you have fine-tuned\n","        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n","        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n","        #device = torch.device(\"cpu\")\n","        model.to(args.device)\n","\n","    # Evaluation\n","    results = {}\n","    if args.do_eval and args.local_rank in [-1, 0]:\n","        checkpoints = [args.output_dir]\n","        if args.eval_all_checkpoints:\n","            checkpoints = list(\n","                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n","            )\n","            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n","            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n","\n","            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n","            model.to(args.device)\n","            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n","            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n","            results.update(result)\n","\n","    return results"],"metadata":{"id":"MjSqe3oxBTCy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["main(trn_df, val_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["fdf8c19483ea4320b8c888924363e9b6","91626fa9d0d1436c9537b1d53760fff5","1ee38cd6ba2d430a8c4c6af47451c570","4c57da17e8be418c8dfc406e542d8b75","c62bdbbbe9a446aaa58db515fca9626c","32bf336850ee4672a863c2e668e384db","767412dc28f24765b210bd2743e0664d","1216c8815bb14f87bdec5fd8878136d7","1b03b10d31554757ad97c6729a3c1f8d","d13cf3a067b848d9903e2b8bd20dbd21","3a042cc3590a4f3e90b6871c3c38c700","e9a9b5e61f2f4e9281d57207ba2ba975","710bd6d8ded1465ea1f786dc50135f8b","43ca978dfaa94ca6ba6cff1e6f6f6e0a","6862ccd007ac4d10bbbc89089d97a51a","96cc8b05725a4a8abdbe784906671e73","1cc3e691979f4d218b801705381cab1d","4279a849ed78474cbe2cef306864b59c","16058d87ce5448548f120f0fa4c0ecbf","228ba6ca82f2485baf1d5930e1738c40","c543bffce19749a0874a2e4ad248aa63","d05648a5be5d4d818fc7d0a0ef922e3c","5e86a98ad638404baf52e09fdf3be896","3eb421597e474585a2461913768c9ee2","e2de32753a66442cb701a6fe83aad91c","c642b1bcedcb43cd99a3f601a7f3f5e3","51588623cfb4474b8bf8d42a444295d9","638cadcc6d464aa6937352786a8e2dd3","cc5361fe2a1c4560835dcd2f3e575f2f","afcf729b57974d778775a857a6fe27b6","6764e5ccc1054c52bee806f3a8d00df0","35dcc3c13a4f431ba784ff10bff06de3","2791828d08ca43d2aee0a9577ceddf4e","4f600f29a234459d950e45e78e88224d","d44231a59672407db71cb2540c8064f2","dcee001970a04badb27fb5c3c249dd6b","e6c34fe75a6547a49948e9b970f1e07f","f1f6f1ee7d874fe480371de65d19a4e8","cc9e2c1f8e0f4433807b982f59c7559b","f8f19792cd6e48d1a248ae0671a367a0","5f5fe2713f10442c93454e33fe4a8920","91cb2751e2944631b43d449d5e3ea664","efd3b95ae8104fe2b84438618a118f42","6abc6ebd2e9143fc96393d86e5ddc669","a9d239a185f7443fb8cd3ba536f3bcd0","c59999d0942b4952b45201e384697894","9001231380914348b4afea934feb2074","4bc3a5693a82453cb192b97a583fa2b6","bda4d2fd4b7e48debad663eb5ec598ad","1c199050148a4b4bb9d0f1bcb01f4433","606e02c14ed94a348a311f12d32342d5","81e5cf0372714f58855211a8c0c341a4","09bfdbd82a344a8d8205f1b8cf433829","00c8eb775a5946dd91f550404e05ebd3","a4587585fbc04d4f973f1eedc215b7fb","79831c710e134ea9b8830a95ad623e79","74dafb5e1f8449dbb05d5bbdc21fc428","6cc008309c564b8585b84e86c73f7b24","1027c10ae2094ddab639cf4bd6699b71","f77e9c9abb0c4c37897f96f18d62512f","12ffe1f1d2884b3c8b18c1a9ff4a8d16","e54ebe9c50e543e9bdc399ec04eff2b1","f7a7a8de4ba44bd89d108cbb585a8816","b4fa2faa26e440719d10293e5187c06e","77fb5dbcf46646c2b74d41ef14809953","470bde09cd724070b41457af0982a91b"]},"id":"brPIphJbBUS1","executionInfo":{"status":"ok","timestamp":1645062727106,"user_tz":-60,"elapsed":4604844,"user":{"displayName":"Ali Kemal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06628100712125702404"}},"outputId":"4ba05fcc-9bf2-4fe6-b9ec-4f33a7efec89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["02/17/2022 00:35:22 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:807: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  FutureWarning,\n","02/17/2022 00:35:29 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x7f01f18665d0>\n","02/17/2022 00:35:29 - INFO - __main__ -   Creating features from dataset file at cached\n","02/17/2022 00:35:45 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","02/17/2022 00:35:45 - INFO - __main__ -   ***** Running training *****\n","02/17/2022 00:35:45 - INFO - __main__ -     Num examples = 10620\n","02/17/2022 00:35:45 - INFO - __main__ -     Num Epochs = 4\n","02/17/2022 00:35:45 - INFO - __main__ -     Instantaneous batch size per GPU = 1\n","02/17/2022 00:35:45 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 1\n","02/17/2022 00:35:45 - INFO - __main__ -     Gradient Accumulation steps = 1\n","02/17/2022 00:35:45 - INFO - __main__ -     Total optimization steps = 42480\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fdf8c19483ea4320b8c888924363e9b6","version_minor":0,"version_major":2},"text/plain":["Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9a9b5e61f2f4e9281d57207ba2ba975","version_minor":0,"version_major":2},"text/plain":["Iteration:   0%|          | 0/10620 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","02/17/2022 00:41:50 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-3500\n","02/17/2022 00:42:05 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-3500\n","02/17/2022 00:48:13 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-7000\n","02/17/2022 00:48:27 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-7000\n","02/17/2022 00:54:36 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-10500\n","02/17/2022 00:54:50 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-10500\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e86a98ad638404baf52e09fdf3be896","version_minor":0,"version_major":2},"text/plain":["Iteration:   0%|          | 0/10620 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["02/17/2022 01:00:57 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-14000\n","02/17/2022 01:01:10 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-14000\n","02/17/2022 01:07:16 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-17500\n","02/17/2022 01:07:29 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-17500\n","02/17/2022 01:13:37 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-21000\n","02/17/2022 01:13:51 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-21000\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f600f29a234459d950e45e78e88224d","version_minor":0,"version_major":2},"text/plain":["Iteration:   0%|          | 0/10620 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["02/17/2022 01:19:52 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-24500\n","02/17/2022 01:19:55 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-24500\n","02/17/2022 01:25:59 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-28000\n","02/17/2022 01:26:03 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-28000\n","02/17/2022 01:32:05 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-31500\n","02/17/2022 01:32:09 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-31500\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a9d239a185f7443fb8cd3ba536f3bcd0","version_minor":0,"version_major":2},"text/plain":["Iteration:   0%|          | 0/10620 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["02/17/2022 01:38:11 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-35000\n","02/17/2022 01:38:15 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-35000\n","02/17/2022 01:44:21 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-38500\n","02/17/2022 01:44:25 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-38500\n","02/17/2022 01:50:31 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-42000\n","02/17/2022 01:50:35 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-42000\n","02/17/2022 01:51:25 - INFO - __main__ -    global_step = 42480, average loss = 2.2371146610739823\n","02/17/2022 01:51:25 - INFO - __main__ -   Saving model checkpoint to output-small\n","02/17/2022 01:51:35 - INFO - __main__ -   Evaluate the following checkpoints: ['output-small']\n","02/17/2022 01:51:38 - INFO - __main__ -   Creating features from dataset file at cached\n","02/17/2022 01:51:40 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n","02/17/2022 01:51:43 - INFO - __main__ -   ***** Running evaluation  *****\n","02/17/2022 01:51:43 - INFO - __main__ -     Num examples = 1181\n","02/17/2022 01:51:43 - INFO - __main__ -     Batch size = 1\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"79831c710e134ea9b8830a95ad623e79","version_minor":0,"version_major":2},"text/plain":["Evaluating:   0%|          | 0/1181 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["02/17/2022 01:52:07 - INFO - __main__ -   ***** Eval results  *****\n","02/17/2022 01:52:07 - INFO - __main__ -     perplexity = tensor(6.8116)\n"]},{"output_type":"execute_result","data":{"text/plain":["{'perplexity_': tensor(6.8116)}"]},"metadata":{},"execution_count":106}]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n","model = AutoModelWithLMHead.from_pretrained('output-small')"],"metadata":{"id":"8iZdU6SiCNEa","executionInfo":{"status":"ok","timestamp":1645062729348,"user_tz":-60,"elapsed":2260,"user":{"displayName":"Ali Kemal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06628100712125702404"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8b54d999-4865-4762-83f6-8d2eba3bdf24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:807: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["# Let's chat for 15 lines\n","for step in range(15):\n","    # encode the new user input, add the eos_token and return a tensor in Pytorch\n","    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n","    # print(new_user_input_ids)\n","\n","    # append the new user input tokens to the chat history\n","    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n","\n","    # generated a response while limiting the total chat history to 1000 tokens, \n","    chat_history_ids = model.generate(\n","        bot_input_ids, max_length=200,\n","        pad_token_id=tokenizer.eos_token_id,  \n","        no_repeat_ngram_size=3,       \n","        do_sample=True, \n","        top_k=100, \n","        top_p=0.7,\n","        temperature=0.8\n","    )\n","    \n","    # pretty print last ouput tokens from bot\n","    print(\"Michael: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"],"metadata":{"id":"J2vMru4AaBN4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  "],"metadata":{"id":"e3B8iO9oaD1J"},"execution_count":null,"outputs":[]}]}